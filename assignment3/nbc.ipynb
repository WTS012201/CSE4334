{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "datapath = \"~/Documents/datasets/ford_sentence/data.csv\"\n",
    "data = pd.read_csv(datapath, skip_blank_lines=True)\n",
    "data = data.dropna(how=\"any\")\n",
    "size = data.shape[0]\n",
    "train_data = data.loc[:int(0.6 * size)]\n",
    "val_data = data.loc[int(0.6 * size):int(0.8 * size)]\n",
    "test_data = data.loc[int(0.8 * size):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.Series(vocab_class['Responsibility'])\n",
    "#print(\"P(w | Responsibility)\")\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBC():\n",
    "    def __init__(self, train_data, smoothing=True) -> None:\n",
    "        self.regex = re.compile('[^a-zA-Z ]')\n",
    "        self.class_tables = self.separate_by_class(train_data)\n",
    "        self.priors = self.compute_priors(train_data)\n",
    "        self.occurences = self.compute_occurences()\n",
    "\n",
    "    def separate_by_class(self, train_data):\n",
    "        class_tables = {}\n",
    "\n",
    "        for type in train_data.Type:\n",
    "            if type not in class_tables:\n",
    "                class_tables[type] = train_data.loc[train_data.Type == type]\n",
    "        \n",
    "        return class_tables\n",
    "\n",
    "    def compute_priors(self, train_data):\n",
    "        priors = {}\n",
    "\n",
    "        for class_type, class_data in self.class_tables.items():\n",
    "            priors[class_type] = class_data.shape[0] / train_data.shape[0]\n",
    "        \n",
    "        return priors\n",
    "    \n",
    "    def compute_occurences(self):\n",
    "        vocab_class = {}\n",
    "\n",
    "        for class_type, class_data in self.class_tables.items():\n",
    "            vocab_class[class_type] = {}\n",
    "            for sentence in class_data.New_Sentence:\n",
    "                sentence = sentence.lower()\n",
    "                sentence = self.regex.sub(' ', sentence)\n",
    "                for word in sentence.split():\n",
    "                    if word in vocab_class[class_type]:\n",
    "                        vocab_class[class_type][word] += 1\n",
    "                    else:\n",
    "                        vocab_class[class_type][word] = 1\n",
    "            omitted_vocab = {key : 0 for key, val in vocab_class[class_type].items() if val >= 5}\n",
    "            vocab_class[class_type] = omitted_vocab\n",
    "\n",
    "        for class_type, class_data in self.class_tables.items():\n",
    "            class_size = len(class_data.New_Sentence)\n",
    "            vocab = vocab_class[class_type]\n",
    "\n",
    "            for sentence in class_data.New_Sentence:\n",
    "                added = set()\n",
    "                sentence = sentence.lower()\n",
    "                sentence = self.regex.sub(' ', sentence)\n",
    "                for word in sentence.split():\n",
    "                    if word in vocab.keys() and word not in added:\n",
    "                        vocab[word] += 1        \n",
    "                    added.add(word)\n",
    "            for word in vocab.keys():\n",
    "                vocab[word] /= class_size\n",
    "        \n",
    "        return vocab_class\n",
    "\n",
    "    def __call__(self, sentence):\n",
    "        max_a_posteriori = (None, 0)\n",
    "\n",
    "        for class_type, prior in self.priors.items():\n",
    "            sentence = sentence.lower()\n",
    "            sentence = self.regex.sub(' ', sentence)\n",
    "            likelihood = prior\n",
    "            for word in sentence.split():\n",
    "                if word in self.occurences[class_type]:\n",
    "                    likelihood *= self.occurences[class_type][word] \n",
    "                else:\n",
    "                    likelihood *= 1 / len(self.occurences[class_type].keys())\n",
    "\n",
    "#            print(f\"{class_type}: {likelihood}\")\n",
    "            if likelihood > max_a_posteriori[1]:\n",
    "                max_a_posteriori = (class_type, likelihood)\n",
    "        \n",
    "        return max_a_posteriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NBC(train_data, smoothing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, data):\n",
    "    score = 0\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "        pred, _ = model(data.iloc[i].New_Sentence)\n",
    "        score += pred == data.iloc[i].Type\n",
    "\n",
    "    return score / data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6530788496415926"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(model, val_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
